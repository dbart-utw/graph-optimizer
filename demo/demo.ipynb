{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph-Optimizer Demo\n",
    "\n",
    "## Tool description\n",
    "In short, the Graph-Optimizer tool performs the following functions:\n",
    "- Predicts the execution time (in milliseconds) and energy consumption (in Joules) for a given BGO or DAG of BGOs on a specific hardware configuration.\n",
    "- Returns the model in symbolical form with graph properties as symbols or predicts execution times if the graph properties are specified.\n",
    "- This is done via an API where issuing a POST request to `<api_url>/models` with the BGO DAG and hardware configuration returns an annotated DAG with calibrated symbolical models. Calling `<api_url>/evaluate` with the BGO DAG, hardware configuration, and graph properties returns an annotated DAG with predicted execution times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What is a BGO?\n",
    "A BGO, or Basic Graph Operation, is an atomic graph operation, that can serve as a building block for constructing larger graph _workloads_.\n",
    "A single BGO can have multiple implementations, possibly targeting different hardware platforms (e.g., CPU or GPU).\n",
    "A workload is a Directed Acyclic Graph (DAG) of BGOs, where the nodes are BGOs and the edges represent data dependencies between them.\n",
    "An example of such a DAG is shown below:\n",
    "\n",
    "<img style=\"margin-bottom: -245px\" src=\"../images/dag.svg\">\n",
    "\n",
    "In this DAG, we start with the Betweenness Centrality (BC) BGO, followed by the Breadth First Search (BFS) and Find Max BGO's. Finally, we have the Find Path BGO to conclude. This example is a workload that outputs the path from the root node to the node with the highest betweenness centrality, which is the most popular node in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance modeling\n",
    "\n",
    "Each implementation of a BGO has a symbolic model that describes its execution time and energy consumption as functions of graph properties and hardware characteristics. Specifically, these hardware characteristics refer to the execution times of atomic operations in hardware, or operations considered to be atomic, such as reading a value from memory, writing a value to memory, performing an integer addition, and so on. These characteristics are obtained through _microbenchmarks_ run on the hardware where the BGO will be executed. The values obtained from the microbenchmarks are used to calibrate the symbolic models, which then provide the execution time and energy consumption of the BGO based solely on the graph properties. This approach allows for predicting the execution time and energy consumption of a BGO on a specific hardware configuration without needing to run the BGO on the hardware for any particular input graph.\n",
    "\n",
    "Such a calibrated model might look like this:\n",
    "\n",
    "$T_{BGO} = 561n \\times 924m + 91n^2$,\n",
    "\n",
    "where $n$ is the number of nodes in a graph, $m$ is the number of edges in the graph, and $T_{BGO}$ is the execution time of the BGO in milliseconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "The following are the requirements to run this Graph-Optimizer demo.\n",
    "You can either install them manually using the commands provided, or run the following code cell, which will install them automatically:\n",
    "- **Flask**: The prediction server makes use of _Flask_ to serve the API. Install it using `pip install Flask`.\n",
    "- **ipywidgets**: This notebook uses _ipywidgets_ for interactive experiments. Install it using `pip install ipywidgets`.\n",
    "- **matplotlib**: This notebook uses _matplotlib_ for plotting. Install it using `pip install matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n"
     ]
    }
   ],
   "source": [
    "# Install requirements\n",
    "# Flask is required to run the prediction API\n",
    "!pip install Flask\n",
    "# Widgets are used later in this file for interacting with the data\n",
    "!pip install ipywidgets\n",
    "# Matplotlib is used for plotting\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to use the Graph-Optimizer tool\n",
    "### Step 1: Specify input DAG of BGO's\n",
    "\n",
    "The first step in using Graph Optimizer is to specify the input DAG of BGO's. From the above list, select the BGO's you want to use and specify the input DAG.\n",
    "This DAG should include one or multiple BGOs and their dependencies. The BGO name should match the name of the BGO folder in the `models` directory.\n",
    "There are currently four BGO's available. These are:\n",
    "- bc (Betweenness Centrality)\n",
    "- bfs (Breadth First Search)\n",
    "- find_max (Find Max)\n",
    "- find_path (Find Path)\n",
    "\n",
    "The dependencies should be specified as a list of BGO id's that the current BGO depends on. For instance, consider the following example with multiple BGOs and dependencies, representing the DAG shown in the introduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_dag = [\n",
    "    {\n",
    "        \"id\": 0,\n",
    "        \"name\": \"pr\",\n",
    "        \"dependencies\": []\n",
    "    },\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"name\": \"find_max\",\n",
    "        \"dependencies\": [0]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"name\": \"bfs\",\n",
    "        \"dependencies\": [0]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"name\": \"find_path\",\n",
    "        \"dependencies\": [1,2]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Step 2: Specify hardware configuration\n",
    "The hardware configuration aims to describe all available hardware components in a system or data center. The hardware information is used for the calibration of the performance models.\n",
    "\n",
    "To specify a custom hardware configuration, you need to provide the configuration in JSON format. This configuration should list all unique available hosts in the data center, including details about CPUs and, if applicable, GPUs. Running microbenchmarks is part of this process and is done automatically with a script. An example configuration is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hardware = {\n",
    "    \"hosts\": [\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"name\": \"H01\",\n",
    "            \"cpus\": {\n",
    "                \"id\": 1,\n",
    "                \"name\": \"intel xeon\",\n",
    "                \"clock_speed\": 2.10,\n",
    "                \"cores\": 16,\n",
    "                \"threads\": 32,\n",
    "                \"wattage\": 35,\n",
    "                \"amount\": 2,\n",
    "                \"benchmarks\": {\n",
    "                    \"T_int_add\": 2.4, \"T_float_gt\": 0.8,\n",
    "                    \"T_q_pop\": 11.2, \"T_q_push\": 16.1, \"T_q_front\": 14.5,\n",
    "                    \"T_L1_read\": 1.26, \"T_L2_read\": 4.24, \"T_L3_read\": 20.9, \"T_DRAM_read\": 62.5,\n",
    "                    \"L1_linesize\": 64, \"L2_linesize\": 64, \"L3_linesize\": 64,\n",
    "                    \"T_heap_insert_max\": 52.7, \"T_heap_extract_min\": 123.3, \"T_heap_decrease_key\": 12.7,\n",
    "                    \"T_push_back\": 12\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining hardware information\n",
    "\n",
    "The following code cell will fetch the hardware information for the system that this notebook is running on, and fill it in in the hardware configuration JSON object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "lscpu = {k.strip(): v.strip() for item in subprocess.check_output(['lscpu']).decode('utf-8').split('\\n') for k, _, v in [item.partition(':')]}\n",
    "hardware['hosts'][0]['cpus']['name'] = lscpu['Model name']\n",
    "hardware['hosts'][0]['cpus']['clock_speed'] = float(lscpu['CPU max MHz']) / 1000\n",
    "hardware['hosts'][0]['cpus']['cores'] = int(lscpu['Core(s) per socket'])\n",
    "hardware['hosts'][0]['cpus']['threads'] = int(lscpu['Thread(s) per core']) * hardware['hosts'][0]['cpus']['cores']\n",
    "hardware['hosts'][0]['cpus']['amount'] = int(lscpu['Socket(s)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### Automated microbenchmarks\n",
    "\n",
    "Running the following python cell will run the microbenchmarks on your machine (this should take a couple of seconds, probably no longer than a minute), and insert them into the hardware configuration.\n",
    "\n",
    "The resulting values are the measured values for each operation in nanoseconds.\n",
    "\n",
    "**Note**: for running the microbenchmarks, g++ is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running microbenchmarks...\n",
      "{\n",
      "    \"T_int_add\": 1.783689,\n",
      "    \"T_int_mult\": 0.2884912,\n",
      "    \"T_int_gt\": 0.1173626,\n",
      "    \"T_int_neq\": 0.2306789,\n",
      "    \"T_float_add\": 0.4410961,\n",
      "    \"T_float_sub\": 0.4404303,\n",
      "    \"T_float_mult\": 0.4289227,\n",
      "    \"T_float_div\": 1.015759,\n",
      "    \"T_float_gt\": 0.113075,\n",
      "    \"T_q_push\": 10.36107,\n",
      "    \"T_q_front\": 9.786193,\n",
      "    \"T_q_pop\": 7.891477,\n",
      "    \"T_heap_insert_max\": 33.04297,\n",
      "    \"T_heap_extract_min\": 72.06602,\n",
      "    \"T_heap_decrease_key\": 8.516269,\n",
      "    \"T_push_back\": 9.859341,\n",
      "    \"L1_linesize\": 64,\n",
      "    \"L2_linesize\": 64,\n",
      "    \"L3_linesize\": 64,\n",
      "    \"T_L1_read\": 1.4027926795578587,\n",
      "    \"T_L2_read\": 7.254708423623955,\n",
      "    \"T_L3_read\": 24.627908775186604,\n",
      "    \"T_DRAM_read\": 111.61792437058531\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from benchmarks.microbenchmarks import all_benchmarks\n",
    "import json\n",
    "\n",
    "# Run microbenchmarks on this machine\n",
    "local_benchmarks = all_benchmarks()\n",
    "\n",
    "# Pretty print the benchmarks\n",
    "print(json.dumps(local_benchmarks, indent=4))\n",
    "\n",
    "# Assign obtained values to the hardware description\n",
    "hardware[\"hosts\"][0][\"cpus\"][\"benchmarks\"] = local_benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run the prediction server\n",
    "\n",
    "The next step is running the prediction server, and getting the performance models for the input DAG for your specific hardware configurations.\n",
    "This step can be divided into two subtasks. First we start the prediction server, and then we submit a POST request to the server to get the performance models.\n",
    "\n",
    "1. Start the server using flask, by running the following command from the root directory:\n",
    "    ```bash\n",
    "    flask --app api/api.py run\n",
    "    ```\n",
    "    _(If you are using windows, use `python -m flask --app api/api.py run`)_\n",
    "\n",
    "    This will start the server on `localhost:5000`. Use the following command to start the server on a different port:\n",
    "    ```bash\n",
    "    flask --app api/api.py run --port <port_number>\n",
    "    ```\n",
    "    You can either do this step, or run the python cell below, which will start the server for you.\n",
    "    - **Note**: If you run the server via the cell below, make sure to wait for the server to start before proceeding to the next cells.\n",
    "    - If you are finished with the experiments, you can stop the server by running the last cell in this document, which will terminate the server.\n",
    "2. Run the prediction by submitting a POST request to the api\n",
    "    - For obtaining the calibrated symbolical models, issue a post request to `localhost:<port_number>/models`, with the following post data:\n",
    "        - \"input_dag\": The input BGO DAG in JSON format, as a string.\n",
    "        - \"hardware\": The hardware configuration in JSON format, as a string.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'api/api.py'\n",
      " * Debug mode: off\n",
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"H01\", \"cpus\": {\"id\": 1, \"name\": \"AMD Ryzen 7 PRO 6850H with Radeon Graphics\", \"clock_speed\": 4.785, \"cores\": 8, \"threads\": 16, \"wattage\": 35, \"amount\": 1, \"benchmarks\": {\"T_int_add\": 1.783689, \"T_int_mult\": 0.2884912, \"T_int_gt\": 0.1173626, \"T_int_neq\": 0.2306789, \"T_float_add\": 0.4410961, \"T_float_sub\": 0.4404303, \"T_float_mult\": 0.4289227, \"T_float_div\": 1.015759, \"T_float_gt\": 0.113075, \"T_q_push\": 10.36107, \"T_q_front\": 9.786193, \"T_q_pop\": 7.891477, \"T_heap_insert_max\": 33.04297, \"T_heap_extract_min\": 72.06602, \"T_heap_decrease_key\": 8.516269, \"T_push_back\": 9.859341, \"L1_linesize\": 64, \"L2_linesize\": 64, \"L3_linesize\": 64, \"T_L1_read\": 1.4027926795578587, \"T_L2_read\": 7.254708423623955, \"T_L3_read\": 24.627908775186604, \"T_DRAM_read\": 111.61792437058531}}}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'actor-collaboration'), ('greenify', 'true')])\n",
      "actor-collaboration\n",
      "127.0.0.1 - - [29/Jun/2025 20:22:19] \"POST /benchmark HTTP/1.1\" 200 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"H01\", \"cpus\": {\"id\": 1, \"name\": \"AMD Ryzen 7 PRO 6850H with Radeon Graphics\", \"clock_speed\": 4.785, \"cores\": 8, \"threads\": 16, \"wattage\": 35, \"amount\": 1, \"benchmarks\": {\"T_int_add\": 1.783689, \"T_int_mult\": 0.2884912, \"T_int_gt\": 0.1173626, \"T_int_neq\": 0.2306789, \"T_float_add\": 0.4410961, \"T_float_sub\": 0.4404303, \"T_float_mult\": 0.4289227, \"T_float_div\": 1.015759, \"T_float_gt\": 0.113075, \"T_q_push\": 10.36107, \"T_q_front\": 9.786193, \"T_q_pop\": 7.891477, \"T_heap_insert_max\": 33.04297, \"T_heap_extract_min\": 72.06602, \"T_heap_decrease_key\": 8.516269, \"T_push_back\": 9.859341, \"L1_linesize\": 64, \"L2_linesize\": 64, \"L3_linesize\": 64, \"T_L1_read\": 1.4027926795578587, \"T_L2_read\": 7.254708423623955, \"T_L3_read\": 24.627908775186604, \"T_DRAM_read\": 111.61792437058531}}}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'amazon0601'), ('greenify', 'true')])\n",
      "amazon0601\n",
      "127.0.0.1 - - [29/Jun/2025 20:22:19] \"POST /benchmark HTTP/1.1\" 200 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"H01\", \"cpus\": {\"id\": 1, \"name\": \"AMD Ryzen 7 PRO 6850H with Radeon Graphics\", \"clock_speed\": 4.785, \"cores\": 8, \"threads\": 16, \"wattage\": 35, \"amount\": 1, \"benchmarks\": {\"T_int_add\": 1.783689, \"T_int_mult\": 0.2884912, \"T_int_gt\": 0.1173626, \"T_int_neq\": 0.2306789, \"T_float_add\": 0.4410961, \"T_float_sub\": 0.4404303, \"T_float_mult\": 0.4289227, \"T_float_div\": 1.015759, \"T_float_gt\": 0.113075, \"T_q_push\": 10.36107, \"T_q_front\": 9.786193, \"T_q_pop\": 7.891477, \"T_heap_insert_max\": 33.04297, \"T_heap_extract_min\": 72.06602, \"T_heap_decrease_key\": 8.516269, \"T_push_back\": 9.859341, \"L1_linesize\": 64, \"L2_linesize\": 64, \"L3_linesize\": 64, \"T_L1_read\": 1.4027926795578587, \"T_L2_read\": 7.254708423623955, \"T_L3_read\": 24.627908775186604, \"T_DRAM_read\": 111.61792437058531}}}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'flixster'), ('greenify', 'true')])\n",
      "flixster\n",
      "127.0.0.1 - - [29/Jun/2025 20:22:19] \"POST /benchmark HTTP/1.1\" 200 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"H01\", \"cpus\": {\"id\": 1, \"name\": \"AMD Ryzen 7 PRO 6850H with Radeon Graphics\", \"clock_speed\": 4.785, \"cores\": 8, \"threads\": 16, \"wattage\": 35, \"amount\": 1, \"benchmarks\": {\"T_int_add\": 1.783689, \"T_int_mult\": 0.2884912, \"T_int_gt\": 0.1173626, \"T_int_neq\": 0.2306789, \"T_float_add\": 0.4410961, \"T_float_sub\": 0.4404303, \"T_float_mult\": 0.4289227, \"T_float_div\": 1.015759, \"T_float_gt\": 0.113075, \"T_q_push\": 10.36107, \"T_q_front\": 9.786193, \"T_q_pop\": 7.891477, \"T_heap_insert_max\": 33.04297, \"T_heap_extract_min\": 72.06602, \"T_heap_decrease_key\": 8.516269, \"T_push_back\": 9.859341, \"L1_linesize\": 64, \"L2_linesize\": 64, \"L3_linesize\": 64, \"T_L1_read\": 1.4027926795578587, \"T_L2_read\": 7.254708423623955, \"T_L3_read\": 24.627908775186604, \"T_DRAM_read\": 111.61792437058531}}}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'higgs_social_network'), ('greenify', 'true')])\n",
      "higgs_social_network\n",
      "127.0.0.1 - - [29/Jun/2025 20:22:19] \"POST /benchmark HTTP/1.1\" 200 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"H01\", \"cpus\": {\"id\": 1, \"name\": \"AMD Ryzen 7 PRO 6850H with Radeon Graphics\", \"clock_speed\": 4.785, \"cores\": 8, \"threads\": 16, \"wattage\": 35, \"amount\": 1, \"benchmarks\": {\"T_int_add\": 1.783689, \"T_int_mult\": 0.2884912, \"T_int_gt\": 0.1173626, \"T_int_neq\": 0.2306789, \"T_float_add\": 0.4410961, \"T_float_sub\": 0.4404303, \"T_float_mult\": 0.4289227, \"T_float_div\": 1.015759, \"T_float_gt\": 0.113075, \"T_q_push\": 10.36107, \"T_q_front\": 9.786193, \"T_q_pop\": 7.891477, \"T_heap_insert_max\": 33.04297, \"T_heap_extract_min\": 72.06602, \"T_heap_decrease_key\": 8.516269, \"T_push_back\": 9.859341, \"L1_linesize\": 64, \"L2_linesize\": 64, \"L3_linesize\": 64, \"T_L1_read\": 1.4027926795578587, \"T_L2_read\": 7.254708423623955, \"T_L3_read\": 24.627908775186604, \"T_DRAM_read\": 111.61792437058531}}}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'patentcite'), ('greenify', 'true')])\n",
      "patentcite\n",
      "127.0.0.1 - - [29/Jun/2025 20:22:20] \"POST /benchmark HTTP/1.1\" 200 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"H01\", \"cpus\": {\"id\": 1, \"name\": \"AMD Ryzen 7 PRO 6850H with Radeon Graphics\", \"clock_speed\": 4.785, \"cores\": 8, \"threads\": 16, \"wattage\": 35, \"amount\": 1, \"benchmarks\": {\"T_int_add\": 1.783689, \"T_int_mult\": 0.2884912, \"T_int_gt\": 0.1173626, \"T_int_neq\": 0.2306789, \"T_float_add\": 0.4410961, \"T_float_sub\": 0.4404303, \"T_float_mult\": 0.4289227, \"T_float_div\": 1.015759, \"T_float_gt\": 0.113075, \"T_q_push\": 10.36107, \"T_q_front\": 9.786193, \"T_q_pop\": 7.891477, \"T_heap_insert_max\": 33.04297, \"T_heap_extract_min\": 72.06602, \"T_heap_decrease_key\": 8.516269, \"T_push_back\": 9.859341, \"L1_linesize\": 64, \"L2_linesize\": 64, \"L3_linesize\": 64, \"T_L1_read\": 1.4027926795578587, \"T_L2_read\": 7.254708423623955, \"T_L3_read\": 24.627908775186604, \"T_DRAM_read\": 111.61792437058531}}}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'sx_stackoverflow_c2q'), ('greenify', 'true')])\n",
      "sx_stackoverflow_c2q\n",
      "127.0.0.1 - - [29/Jun/2025 20:22:20] \"POST /benchmark HTTP/1.1\" 200 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"H01\", \"cpus\": {\"id\": 1, \"name\": \"AMD Ryzen 7 PRO 6850H with Radeon Graphics\", \"clock_speed\": 4.785, \"cores\": 8, \"threads\": 16, \"wattage\": 35, \"amount\": 1, \"benchmarks\": {\"T_int_add\": 1.783689, \"T_int_mult\": 0.2884912, \"T_int_gt\": 0.1173626, \"T_int_neq\": 0.2306789, \"T_float_add\": 0.4410961, \"T_float_sub\": 0.4404303, \"T_float_mult\": 0.4289227, \"T_float_div\": 1.015759, \"T_float_gt\": 0.113075, \"T_q_push\": 10.36107, \"T_q_front\": 9.786193, \"T_q_pop\": 7.891477, \"T_heap_insert_max\": 33.04297, \"T_heap_extract_min\": 72.06602, \"T_heap_decrease_key\": 8.516269, \"T_push_back\": 9.859341, \"L1_linesize\": 64, \"L2_linesize\": 64, \"L3_linesize\": 64, \"T_L1_read\": 1.4027926795578587, \"T_L2_read\": 7.254708423623955, \"T_L3_read\": 24.627908775186604, \"T_DRAM_read\": 111.61792437058531}}}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'actor-collaboration'), ('greenify', 'true')])\n",
      "actor-collaboration\n",
      "[2025-06-29 20:22:49,017] ERROR in app: Exception on /benchmark [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/flask/app.py\", line 1463, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/flask/app.py\", line 872, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/flask/app.py\", line 870, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/flask/app.py\", line 855, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/duncan/ownCloud/Documents/PhD/graph-optimizer/api/api.py\", line 65, in benchmark\n",
      "    return greenify_output(output)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/duncan/ownCloud/Documents/PhD/graph-optimizer/api/api.py\", line 35, in greenify_output\n",
      "    greenifier_output = {\"tasks\": json.loads(output)}\n",
      "                                  ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/json/__init__.py\", line 339, in loads\n",
      "    raise TypeError(f'the JSON object must be str, bytes or bytearray, '\n",
      "TypeError: the JSON object must be str, bytes or bytearray, not tuple\n",
      "127.0.0.1 - - [29/Jun/2025 20:22:49] \"\u001b[35m\u001b[1mPOST /benchmark HTTP/1.1\u001b[0m\" 500 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"H01\", \"cpus\": {\"id\": 1, \"name\": \"AMD Ryzen 7 PRO 6850H with Radeon Graphics\", \"clock_speed\": 4.785, \"cores\": 8, \"threads\": 16, \"wattage\": 35, \"amount\": 1, \"benchmarks\": {\"T_int_add\": 1.783689, \"T_int_mult\": 0.2884912, \"T_int_gt\": 0.1173626, \"T_int_neq\": 0.2306789, \"T_float_add\": 0.4410961, \"T_float_sub\": 0.4404303, \"T_float_mult\": 0.4289227, \"T_float_div\": 1.015759, \"T_float_gt\": 0.113075, \"T_q_push\": 10.36107, \"T_q_front\": 9.786193, \"T_q_pop\": 7.891477, \"T_heap_insert_max\": 33.04297, \"T_heap_extract_min\": 72.06602, \"T_heap_decrease_key\": 8.516269, \"T_push_back\": 9.859341, \"L1_linesize\": 64, \"L2_linesize\": 64, \"L3_linesize\": 64, \"T_L1_read\": 1.4027926795578587, \"T_L2_read\": 7.254708423623955, \"T_L3_read\": 24.627908775186604, \"T_DRAM_read\": 111.61792437058531}}}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'actor-collaboration'), ('greenify', 'true')])\n",
      "actor-collaboration\n",
      "[2025-06-29 20:22:53,388] ERROR in app: Exception on /benchmark [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/flask/app.py\", line 1463, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/flask/app.py\", line 872, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/flask/app.py\", line 870, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/flask/app.py\", line 855, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/duncan/ownCloud/Documents/PhD/graph-optimizer/api/api.py\", line 65, in benchmark\n",
      "    return greenify_output(output)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/duncan/ownCloud/Documents/PhD/graph-optimizer/api/api.py\", line 35, in greenify_output\n",
      "    greenifier_output = {\"tasks\": json.loads(output)}\n",
      "                                  ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/json/__init__.py\", line 339, in loads\n",
      "    raise TypeError(f'the JSON object must be str, bytes or bytearray, '\n",
      "TypeError: the JSON object must be str, bytes or bytearray, not tuple\n",
      "127.0.0.1 - - [29/Jun/2025 20:22:53] \"\u001b[35m\u001b[1mPOST /benchmark HTTP/1.1\u001b[0m\" 500 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"H01\", \"cpus\": {\"id\": 1, \"name\": \"AMD Ryzen 7 PRO 6850H with Radeon Graphics\", \"clock_speed\": 4.785, \"cores\": 8, \"threads\": 16, \"wattage\": 35, \"amount\": 1, \"benchmarks\": {\"T_int_add\": 1.783689, \"T_int_mult\": 0.2884912, \"T_int_gt\": 0.1173626, \"T_int_neq\": 0.2306789, \"T_float_add\": 0.4410961, \"T_float_sub\": 0.4404303, \"T_float_mult\": 0.4289227, \"T_float_div\": 1.015759, \"T_float_gt\": 0.113075, \"T_q_push\": 10.36107, \"T_q_front\": 9.786193, \"T_q_pop\": 7.891477, \"T_heap_insert_max\": 33.04297, \"T_heap_extract_min\": 72.06602, \"T_heap_decrease_key\": 8.516269, \"T_push_back\": 9.859341, \"L1_linesize\": 64, \"L2_linesize\": 64, \"L3_linesize\": 64, \"T_L1_read\": 1.4027926795578587, \"T_L2_read\": 7.254708423623955, \"T_L3_read\": 24.627908775186604, \"T_DRAM_read\": 111.61792437058531}}}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'actor-collaboration'), ('greenify', 'true')])\n",
      "actor-collaboration\n",
      "[2025-06-29 20:23:06,178] ERROR in app: Exception on /benchmark [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/flask/app.py\", line 1463, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/flask/app.py\", line 872, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/flask/app.py\", line 870, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/flask/app.py\", line 855, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/duncan/ownCloud/Documents/PhD/graph-optimizer/api/api.py\", line 65, in benchmark\n",
      "    return greenify_output(output)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/duncan/ownCloud/Documents/PhD/graph-optimizer/api/api.py\", line 35, in greenify_output\n",
      "    greenifier_output = {\"tasks\": json.loads(output)}\n",
      "                                  ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/json/__init__.py\", line 339, in loads\n",
      "    raise TypeError(f'the JSON object must be str, bytes or bytearray, '\n",
      "TypeError: the JSON object must be str, bytes or bytearray, not tuple\n",
      "127.0.0.1 - - [29/Jun/2025 20:23:06] \"\u001b[35m\u001b[1mPOST /benchmark HTTP/1.1\u001b[0m\" 500 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"H01\", \"cpus\": {\"id\": 1, \"name\": \"AMD Ryzen 7 PRO 6850H with Radeon Graphics\", \"clock_speed\": 4.785, \"cores\": 8, \"threads\": 16, \"wattage\": 35, \"amount\": 1, \"benchmarks\": {\"T_int_add\": 1.783689, \"T_int_mult\": 0.2884912, \"T_int_gt\": 0.1173626, \"T_int_neq\": 0.2306789, \"T_float_add\": 0.4410961, \"T_float_sub\": 0.4404303, \"T_float_mult\": 0.4289227, \"T_float_div\": 1.015759, \"T_float_gt\": 0.113075, \"T_q_push\": 10.36107, \"T_q_front\": 9.786193, \"T_q_pop\": 7.891477, \"T_heap_insert_max\": 33.04297, \"T_heap_extract_min\": 72.06602, \"T_heap_decrease_key\": 8.516269, \"T_push_back\": 9.859341, \"L1_linesize\": 64, \"L2_linesize\": 64, \"L3_linesize\": 64, \"T_L1_read\": 1.4027926795578587, \"T_L2_read\": 7.254708423623955, \"T_L3_read\": 24.627908775186604, \"T_DRAM_read\": 111.61792437058531}}}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'actor-collaboration'), ('greenify', 'true')])\n",
      "actor-collaboration\n",
      "[2025-06-29 20:23:13,041] ERROR in app: Exception on /benchmark [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/flask/app.py\", line 1463, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/flask/app.py\", line 872, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/flask/app.py\", line 870, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/flask/app.py\", line 855, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/duncan/ownCloud/Documents/PhD/graph-optimizer/api/api.py\", line 65, in benchmark\n",
      "    return greenify_output(output)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/duncan/ownCloud/Documents/PhD/graph-optimizer/api/api.py\", line 35, in greenify_output\n",
      "    greenifier_output = {\"tasks\": json.loads(output)}\n",
      "                                  ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/json/__init__.py\", line 339, in loads\n",
      "    raise TypeError(f'the JSON object must be str, bytes or bytearray, '\n",
      "TypeError: the JSON object must be str, bytes or bytearray, not tuple\n",
      "127.0.0.1 - - [29/Jun/2025 20:23:13] \"\u001b[35m\u001b[1mPOST /benchmark HTTP/1.1\u001b[0m\" 500 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"H01\", \"cpus\": {\"id\": 1, \"name\": \"AMD Ryzen 7 PRO 6850H with Radeon Graphics\", \"clock_speed\": 4.785, \"cores\": 8, \"threads\": 16, \"wattage\": 35, \"amount\": 1, \"benchmarks\": {\"T_int_add\": 1.783689, \"T_int_mult\": 0.2884912, \"T_int_gt\": 0.1173626, \"T_int_neq\": 0.2306789, \"T_float_add\": 0.4410961, \"T_float_sub\": 0.4404303, \"T_float_mult\": 0.4289227, \"T_float_div\": 1.015759, \"T_float_gt\": 0.113075, \"T_q_push\": 10.36107, \"T_q_front\": 9.786193, \"T_q_pop\": 7.891477, \"T_heap_insert_max\": 33.04297, \"T_heap_extract_min\": 72.06602, \"T_heap_decrease_key\": 8.516269, \"T_push_back\": 9.859341, \"L1_linesize\": 64, \"L2_linesize\": 64, \"L3_linesize\": 64, \"T_L1_read\": 1.4027926795578587, \"T_L2_read\": 7.254708423623955, \"T_L3_read\": 24.627908775186604, \"T_DRAM_read\": 111.61792437058531}}}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'actor-collaboration'), ('greenify', 'true')])\n",
      "actor-collaboration\n",
      "[2025-06-29 20:23:16,452] ERROR in app: Exception on /benchmark [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/flask/app.py\", line 1463, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/flask/app.py\", line 872, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/flask/app.py\", line 870, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3/dist-packages/flask/app.py\", line 855, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/duncan/ownCloud/Documents/PhD/graph-optimizer/api/api.py\", line 65, in benchmark\n",
      "    return greenify_output(output)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/duncan/ownCloud/Documents/PhD/graph-optimizer/api/api.py\", line 35, in greenify_output\n",
      "    greenifier_output = {\"tasks\": json.loads(output)}\n",
      "                                  ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/json/__init__.py\", line 339, in loads\n",
      "    raise TypeError(f'the JSON object must be str, bytes or bytearray, '\n",
      "TypeError: the JSON object must be str, bytes or bytearray, not tuple\n",
      "127.0.0.1 - - [29/Jun/2025 20:23:16] \"\u001b[35m\u001b[1mPOST /benchmark HTTP/1.1\u001b[0m\" 500 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"Power_Save\"}, {\"id\": 3, \"name\": \"Balanced\"}, {\"id\": 2, \"name\": \"Performance\"}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'actor-collaboration'), ('greenify', 'true')])\n",
      "actor-collaboration\n",
      "127.0.0.1 - - [29/Jun/2025 20:25:40] \"POST /benchmark HTTP/1.1\" 200 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"Power_Save\"}, {\"id\": 3, \"name\": \"Balanced\"}, {\"id\": 2, \"name\": \"Performance\"}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'amazon0601'), ('greenify', 'true')])\n",
      "amazon0601\n",
      "127.0.0.1 - - [29/Jun/2025 20:25:41] \"POST /benchmark HTTP/1.1\" 200 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"Power_Save\"}, {\"id\": 3, \"name\": \"Balanced\"}, {\"id\": 2, \"name\": \"Performance\"}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'flixster'), ('greenify', 'true')])\n",
      "flixster\n",
      "127.0.0.1 - - [29/Jun/2025 20:25:41] \"POST /benchmark HTTP/1.1\" 200 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"Power_Save\"}, {\"id\": 3, \"name\": \"Balanced\"}, {\"id\": 2, \"name\": \"Performance\"}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'higgs_social_network'), ('greenify', 'true')])\n",
      "higgs_social_network\n",
      "127.0.0.1 - - [29/Jun/2025 20:25:41] \"POST /benchmark HTTP/1.1\" 200 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"Power_Save\"}, {\"id\": 3, \"name\": \"Balanced\"}, {\"id\": 2, \"name\": \"Performance\"}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'patentcite'), ('greenify', 'true')])\n",
      "patentcite\n",
      "127.0.0.1 - - [29/Jun/2025 20:25:42] \"POST /benchmark HTTP/1.1\" 200 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"Power_Save\"}, {\"id\": 3, \"name\": \"Balanced\"}, {\"id\": 2, \"name\": \"Performance\"}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'sx_stackoverflow_c2q'), ('greenify', 'true')])\n",
      "sx_stackoverflow_c2q\n",
      "127.0.0.1 - - [29/Jun/2025 20:25:42] \"POST /benchmark HTTP/1.1\" 200 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"Power_Save\"}, {\"id\": 3, \"name\": \"Balanced\"}, {\"id\": 2, \"name\": \"Performance\"}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'actor-collaboration'), ('greenify', 'true')])\n",
      "actor-collaboration\n",
      "127.0.0.1 - - [29/Jun/2025 20:29:52] \"POST /benchmark HTTP/1.1\" 200 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"Power_Save\"}, {\"id\": 3, \"name\": \"Balanced\"}, {\"id\": 2, \"name\": \"Performance\"}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'amazon0601'), ('greenify', 'true')])\n",
      "amazon0601\n",
      "127.0.0.1 - - [29/Jun/2025 20:29:52] \"POST /benchmark HTTP/1.1\" 200 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"Power_Save\"}, {\"id\": 3, \"name\": \"Balanced\"}, {\"id\": 2, \"name\": \"Performance\"}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'flixster'), ('greenify', 'true')])\n",
      "flixster\n",
      "127.0.0.1 - - [29/Jun/2025 20:29:53] \"POST /benchmark HTTP/1.1\" 200 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"Power_Save\"}, {\"id\": 3, \"name\": \"Balanced\"}, {\"id\": 2, \"name\": \"Performance\"}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'higgs_social_network'), ('greenify', 'true')])\n",
      "higgs_social_network\n",
      "127.0.0.1 - - [29/Jun/2025 20:29:53] \"POST /benchmark HTTP/1.1\" 200 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"Power_Save\"}, {\"id\": 3, \"name\": \"Balanced\"}, {\"id\": 2, \"name\": \"Performance\"}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'patentcite'), ('greenify', 'true')])\n",
      "patentcite\n",
      "127.0.0.1 - - [29/Jun/2025 20:29:53] \"POST /benchmark HTTP/1.1\" 200 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"Power_Save\"}, {\"id\": 3, \"name\": \"Balanced\"}, {\"id\": 2, \"name\": \"Performance\"}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'sx_stackoverflow_c2q'), ('greenify', 'true')])\n",
      "sx_stackoverflow_c2q\n",
      "127.0.0.1 - - [29/Jun/2025 20:29:54] \"POST /benchmark HTTP/1.1\" 200 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"Power_Save\"}, {\"id\": 3, \"name\": \"Balanced\"}, {\"id\": 2, \"name\": \"Performance\"}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'actor-collaboration'), ('greenify', 'true')])\n",
      "actor-collaboration\n",
      "127.0.0.1 - - [29/Jun/2025 20:30:03] \"POST /benchmark HTTP/1.1\" 200 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"Power_Save\"}, {\"id\": 3, \"name\": \"Balanced\"}, {\"id\": 2, \"name\": \"Performance\"}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'amazon0601'), ('greenify', 'true')])\n",
      "amazon0601\n",
      "127.0.0.1 - - [29/Jun/2025 20:30:03] \"POST /benchmark HTTP/1.1\" 200 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"Power_Save\"}, {\"id\": 3, \"name\": \"Balanced\"}, {\"id\": 2, \"name\": \"Performance\"}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'flixster'), ('greenify', 'true')])\n",
      "flixster\n",
      "127.0.0.1 - - [29/Jun/2025 20:30:03] \"POST /benchmark HTTP/1.1\" 200 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"Power_Save\"}, {\"id\": 3, \"name\": \"Balanced\"}, {\"id\": 2, \"name\": \"Performance\"}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'higgs_social_network'), ('greenify', 'true')])\n",
      "higgs_social_network\n",
      "127.0.0.1 - - [29/Jun/2025 20:30:04] \"POST /benchmark HTTP/1.1\" 200 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"Power_Save\"}, {\"id\": 3, \"name\": \"Balanced\"}, {\"id\": 2, \"name\": \"Performance\"}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'patentcite'), ('greenify', 'true')])\n",
      "patentcite\n",
      "127.0.0.1 - - [29/Jun/2025 20:30:04] \"POST /benchmark HTTP/1.1\" 200 -\n",
      "ImmutableMultiDict([('hardware', '{\"hosts\": [{\"id\": 1, \"name\": \"Power_Save\"}, {\"id\": 3, \"name\": \"Balanced\"}, {\"id\": 2, \"name\": \"Performance\"}]}'), ('bgo_dag', '[{\"id\": 0, \"name\": \"pr\", \"dependencies\": []}, {\"id\": 1, \"name\": \"find_max\", \"dependencies\": [0]}, {\"id\": 2, \"name\": \"bfs\", \"dependencies\": [0]}, {\"id\": 3, \"name\": \"find_path\", \"dependencies\": [1, 2]}]'), ('graph_benchmarks', 'sx_stackoverflow_c2q'), ('greenify', 'true')])\n",
      "sx_stackoverflow_c2q\n",
      "127.0.0.1 - - [29/Jun/2025 20:30:04] \"POST /benchmark HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# Start the server\n",
    "import multiprocessing\n",
    "\n",
    "port = 5000\n",
    "url = f'http://localhost:{port}/'\n",
    "\n",
    "def run_server():\n",
    "    !flask --app api/api.py run --port {port}\n",
    "\n",
    "server = multiprocessing.Process(target=run_server)\n",
    "server.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### Interpreting the results\n",
    "\n",
    "When submitting the post request to the API, the response will be the input DAG, but annotated with the calibrated symbolical models. The models will be in the form of a string representing a mathematical formula, with the graph properties as parameters.\n",
    "\n",
    "For demonstration purposes, a dropdown and slider are provided below, which allow you to change the microbenchmarking parameters and see the impact they have on the performance models.\n",
    "\n",
    "#### Submit a request to the prediction server by executing the cells below, and observe how the models change when altering the microbenchmarking parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def models_request():\n",
    "    form_data = {\n",
    "        'hardware': json.dumps(hardware),\n",
    "        'bgo_dag': json.dumps(input_dag)\n",
    "    }\n",
    "    models_response = requests.post(url + '/models', data=form_data)\n",
    "    return models_response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Change the value of certain microbenchmarks, and see how they impact the performance models"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cfa187ebb8546c4b2f5f710dc099eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Save configuration', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6579baff95410fb589c329c842d978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Variable', options=('T_int_add', 'T_int_mult', 'T_int_gt', 'T_int_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c606ceb6a604b10ba22c035c7cefdbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.74882, description='Value', max=200.0, step=0.01), Output()), _dom_c…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dropdown and slider functionality\n",
    "from ipywidgets import interact, dlink, Dropdown, FloatSlider, Button\n",
    "from IPython.display import Markdown, display, clear_output\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "clear_output()\n",
    "\n",
    "microbenchmarks = hardware['hosts'][0]['cpus']['benchmarks']\n",
    "microbenchmark_name = list(microbenchmarks.keys())[0]\n",
    "dropdown = Dropdown(options=microbenchmarks.keys(), description='Variable')\n",
    "slider = FloatSlider(min=0, max=200, step=0.01, description='Value', value=microbenchmarks[microbenchmark_name])\n",
    "response = None\n",
    "\n",
    "def save_config(arg):\n",
    "    now = datetime.now().strftime('%Y-%m-%d.%H.%M.%S')\n",
    "    dir_name = f'demo/saved_configs/{now}/'\n",
    "    os.makedirs(dir_name, exist_ok=False)\n",
    "    with open(f'{dir_name}/hardware.json', 'w') as hw_file:\n",
    "        hw_file.write(json.dumps(hardware))\n",
    "    with open(f'{dir_name}/models.json', 'w') as model_file:\n",
    "        model_file.write(json.dumps(response))\n",
    "    print(f'Configuration saved to {dir_name} directory')\n",
    "\n",
    "\n",
    "def set_microbenchmark_name(variable):\n",
    "    global microbenchmark_name\n",
    "    microbenchmark_name = variable\n",
    "\n",
    "\n",
    "def update_slider_value(x):\n",
    "    global microbenchmark_name\n",
    "    microbenchmark_name = x\n",
    "    slider.value = microbenchmarks[x]\n",
    "    return slider.value\n",
    "\n",
    "\n",
    "def update_microbenchmark_value(value):\n",
    "    global response\n",
    "    hardware['hosts'][0]['cpus']['benchmarks'][microbenchmark_name] = value\n",
    "    response = json.loads(models_request())\n",
    "    return response\n",
    "\n",
    "\n",
    "dlink((dropdown, 'value'), (slider, 'value'), update_slider_value)\n",
    "display(Markdown('### Change the value of certain microbenchmarks, and see how they impact the performance models'))\n",
    "save_button = Button(description='Save configuration')\n",
    "save_button.on_click(save_config)\n",
    "display(save_button)\n",
    "interact(set_microbenchmark_name, variable=dropdown)\n",
    "interact(update_microbenchmark_value, value=slider);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Specify graph characteristics _(optional)_\n",
    "\n",
    "The final step is to specify the graph characteristics of a specific graph for which you want to predict the execution time. This can be done by submitting a POST request to the API with the input DAG, hardware configuration, and graph properties. The API will return the input DAG annotated with the predicted execution times.\n",
    "\n",
    "The graph properties are expressed in a simple JSON format of which an example is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_props = {\n",
    "    \"n\": 15763,\n",
    "    \"m\": 171206,\n",
    "    \"average_degree\": 21,\n",
    "    \"directed\": False,\n",
    "    \"weighted\": False,\n",
    "    \"diameter\": 7,\n",
    "    \"clustering_coefficient\": 0.0132526,\n",
    "    \"triangle_count\": 591156,\n",
    "    \"s\": 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Change the number of nodes in the graph, and see how it impacts the performance and energy predictions"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6730a6136094b879f04289be33544a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=15763.0, description='#nodes', max=100000.0, min=100.0, step=1.0), Out…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate_request():\n",
    "    form_data = {\n",
    "        'hardware': json.dumps(hardware),\n",
    "        'bgo_dag': json.dumps(input_dag),\n",
    "        'graph_props': json.dumps(graph_props)\n",
    "    }\n",
    "    evaluate_response = requests.post(url + '/evaluate', data=form_data)\n",
    "    print(evaluate_response.text)\n",
    "\n",
    "def set_num_nodes(value):\n",
    "    graph_props[\"n\"] = value\n",
    "    evaluate_request()\n",
    "\n",
    "display(Markdown('### Change the number of nodes in the graph, and see how it impacts the performance and energy predictions'))\n",
    "interact(set_num_nodes, value=FloatSlider(min=100, max=100000, step=1, description='#nodes', value=graph_props[\"n\"]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware configuration comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "saved_configs = 'demo/saved_configs'\n",
    "\n",
    "\n",
    "responses = {}\n",
    "for dir in os.listdir(saved_configs):\n",
    "    path = os.path.join(saved_configs, dir)\n",
    "    if not os.path.isdir(path):\n",
    "        continue\n",
    "    with open(os.path.join(path, 'hardware.json')) as f:\n",
    "        hardware = f.read()\n",
    "        form_data = {\n",
    "            'hardware': hardware,\n",
    "            'bgo_dag': json.dumps(input_dag),\n",
    "            'graph_props': json.dumps(graph_props)\n",
    "        }\n",
    "        evaluate_response = requests.post(url + '/evaluate', data=form_data)\n",
    "        responses[dir] = (json.loads(evaluate_response.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a bar plot of the BGO's, for each BGO showing the different saved configurations as different bars\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import TABLEAU_COLORS\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def get_bgo_names(responses):\n",
    "    # Get bgo names. Also assert if bgo's are the same for every hardware configuration\n",
    "    bgo_names = None\n",
    "    for dag in responses.values():\n",
    "        new_bgo_names = [bgo['name'] for bgo in dag]\n",
    "\n",
    "        assert bgo_names is None or new_bgo_names == bgo_names, 'BGO names are not the same for every hardware configuration'\n",
    "\n",
    "        bgo_names = new_bgo_names\n",
    "\n",
    "    return bgo_names\n",
    "\n",
    "\n",
    "def get_runtime_per_bgo(annotated_dag):\n",
    "    return [bgo['performances'][0]['runtime'] for bgo in annotated_dag]\n",
    "\n",
    "\n",
    "bgo_names = get_bgo_names(responses)\n",
    "bgo_names.append('cumulative')\n",
    "all_values = {dir: get_runtime_per_bgo(dag) for dir, dag in responses.items()}\n",
    "for key, value in all_values.items():\n",
    "    all_values[key].append(sum(value))\n",
    "all_colors = list(TABLEAU_COLORS.keys())\n",
    "\n",
    "ncols = 4\n",
    "nrows = math.ceil(len(bgo_names) / ncols)\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(20, 5*nrows))\n",
    "axs_flat = axs.flat\n",
    "\n",
    "for i, (ax, bgo) in enumerate(zip(axs_flat, bgo_names)):\n",
    "    ax.set_title(bgo)\n",
    "    if i % ncols == 0:\n",
    "        ax.set_ylabel('Runtime (ms)')\n",
    "    names = all_values.keys()\n",
    "    values = [v[i] for _, v in all_values.items()]\n",
    "    bar_colors = [all_colors[i % len(all_colors)]for i in range(len(names))]\n",
    "    ax.bar(names, values, color=bar_colors)\n",
    "\n",
    "# remove unused plots\n",
    "diff = len(axs_flat)-len(bgo_names)\n",
    "if (diff > 0):\n",
    "    for ax in axs_flat[-diff:]:\n",
    "        ax.set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using benchmark data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'tasks': [{'cpuCount': 1, 'cpuUsage': 10000, 'dependencies': [], 'energyConsumption': {'Performance': 13391000}, 'id': 0, 'memCapacity': 1000000, 'name': 'pr', 'runTimes': {'Performance': 248}, 'submissionTime': '2025-06-29'}, {'cpuCount': 1, 'cpuUsage': 10000, 'dependencies': [0], 'energyConsumption': {'Performance': 101929}, 'id': 1, 'memCapacity': 1000000, 'name': 'find_max', 'runTimes': {'Performance': 2}, 'submissionTime': '2025-06-29'}, {'cpuCount': 1, 'cpuUsage': 10000, 'dependencies': [0], 'energyConsumption': {'Performance': 650471}, 'id': 2, 'memCapacity': 1000000, 'name': 'bfs', 'runTimes': {'Performance': 16}, 'submissionTime': '2025-06-29'}, {'cpuCount': 1, 'cpuUsage': 10000, 'dependencies': [1, 2], 'energyConsumption': {'Performance': 4887}, 'id': 3, 'memCapacity': 1000000, 'name': 'find_path', 'runTimes': {'Performance': 0}, 'submissionTime': '2025-06-29'}]}, {'tasks': [{'cpuCount': 1, 'cpuUsage': 10000, 'dependencies': [], 'energyConsumption': {'Performance': 2989362}, 'id': 0, 'memCapacity': 1000000, 'name': 'pr', 'runTimes': {'Performance': 59}, 'submissionTime': '2025-06-29'}, {'cpuCount': 1, 'cpuUsage': 10000, 'dependencies': [0], 'energyConsumption': {'Performance': 115094}, 'id': 1, 'memCapacity': 1000000, 'name': 'find_max', 'runTimes': {'Performance': 2}, 'submissionTime': '2025-06-29'}, {'cpuCount': 1, 'cpuUsage': 10000, 'dependencies': [0], 'energyConsumption': {'Performance': 803533}, 'id': 2, 'memCapacity': 1000000, 'name': 'bfs', 'runTimes': {'Performance': 18}, 'submissionTime': '2025-06-29'}, {'cpuCount': 1, 'cpuUsage': 10000, 'dependencies': [1, 2], 'energyConsumption': {'Performance': 0}, 'id': 3, 'memCapacity': 1000000, 'name': 'find_path', 'runTimes': {'Performance': 0}, 'submissionTime': '2025-06-29'}]}, {'tasks': [{'cpuCount': 1, 'cpuUsage': 10000, 'dependencies': [], 'energyConsumption': {'Performance': 21718591}, 'id': 0, 'memCapacity': 1000000, 'name': 'pr', 'runTimes': {'Performance': 424}, 'submissionTime': '2025-06-29'}, {'cpuCount': 1, 'cpuUsage': 10000, 'dependencies': [0], 'energyConsumption': {'Performance': 545999}, 'id': 1, 'memCapacity': 1000000, 'name': 'find_max', 'runTimes': {'Performance': 17}, 'submissionTime': '2025-06-29'}, {'cpuCount': 1, 'cpuUsage': 10000, 'dependencies': [0], 'energyConsumption': {'Performance': 2431989}, 'id': 2, 'memCapacity': 1000000, 'name': 'bfs', 'runTimes': {'Performance': 55}, 'submissionTime': '2025-06-29'}, {'cpuCount': 1, 'cpuUsage': 10000, 'dependencies': [1, 2], 'energyConsumption': {'Performance': 22464}, 'id': 3, 'memCapacity': 1000000, 'name': 'find_path', 'runTimes': {'Performance': 0}, 'submissionTime': '2025-06-29'}]}, {'tasks': [{'cpuCount': 1, 'cpuUsage': 10000, 'dependencies': [], 'energyConsumption': {'Performance': 13999245}, 'id': 0, 'memCapacity': 1000000, 'name': 'pr', 'runTimes': {'Performance': 244}, 'submissionTime': '2025-06-29'}, {'cpuCount': 1, 'cpuUsage': 10000, 'dependencies': [0], 'energyConsumption': {'Performance': 140603}, 'id': 1, 'memCapacity': 1000000, 'name': 'find_max', 'runTimes': {'Performance': 2}, 'submissionTime': '2025-06-29'}, {'cpuCount': 1, 'cpuUsage': 10000, 'dependencies': [0], 'energyConsumption': {'Performance': 812322}, 'id': 2, 'memCapacity': 1000000, 'name': 'bfs', 'runTimes': {'Performance': 19}, 'submissionTime': '2025-06-29'}, {'cpuCount': 1, 'cpuUsage': 10000, 'dependencies': [1, 2], 'energyConsumption': {'Performance': 604}, 'id': 3, 'memCapacity': 1000000, 'name': 'find_path', 'runTimes': {'Performance': 0}, 'submissionTime': '2025-06-29'}]}, {'tasks': [{'cpuCount': 1, 'cpuUsage': 10000, 'dependencies': [], 'energyConsumption': {'Performance': 45794404}, 'id': 0, 'memCapacity': 1000000, 'name': 'pr', 'runTimes': {'Performance': 923}, 'submissionTime': '2025-06-29'}, {'cpuCount': 1, 'cpuUsage': 10000, 'dependencies': [0], 'energyConsumption': {'Performance': 713341}, 'id': 1, 'memCapacity': 1000000, 'name': 'find_max', 'runTimes': {'Performance': 25}, 'submissionTime': '2025-06-29'}, {'cpuCount': 1, 'cpuUsage': 10000, 'dependencies': [0], 'energyConsumption': {'Performance': 5935299}, 'id': 2, 'memCapacity': 1000000, 'name': 'bfs', 'runTimes': {'Performance': 136}, 'submissionTime': '2025-06-29'}, {'cpuCount': 1, 'cpuUsage': 10000, 'dependencies': [1, 2], 'energyConsumption': {'Performance': 32247}, 'id': 3, 'memCapacity': 1000000, 'name': 'find_path', 'runTimes': {'Performance': 0}, 'submissionTime': '2025-06-29'}]}, {'tasks': [{'cpuCount': 1, 'cpuUsage': 10000, 'dependencies': [], 'energyConsumption': {'Performance': 14569978}, 'id': 0, 'memCapacity': 1000000, 'name': 'pr', 'runTimes': {'Performance': 256}, 'submissionTime': '2025-06-29'}, {'cpuCount': 1, 'cpuUsage': 10000, 'dependencies': [0], 'energyConsumption': {'Performance': 379373}, 'id': 1, 'memCapacity': 1000000, 'name': 'find_max', 'runTimes': {'Performance': 12}, 'submissionTime': '2025-06-29'}, {'cpuCount': 1, 'cpuUsage': 10000, 'dependencies': [0], 'energyConsumption': {'Performance': 1621216}, 'id': 2, 'memCapacity': 1000000, 'name': 'bfs', 'runTimes': {'Performance': 35}, 'submissionTime': '2025-06-29'}, {'cpuCount': 1, 'cpuUsage': 10000, 'dependencies': [1, 2], 'energyConsumption': {'Performance': 14045}, 'id': 3, 'memCapacity': 1000000, 'name': 'find_path', 'runTimes': {'Performance': 0}, 'submissionTime': '2025-06-29'}]}]\n"
     ]
    }
   ],
   "source": [
    "hardware = {\n",
    "    \"hosts\": [\n",
    "        {\"id\": 1, \"name\": \"Power_Save\"},\n",
    "        {\"id\": 3, \"name\": \"Balanced\"},\n",
    "        {\"id\": 2, \"name\": \"Performance\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "def combine_workflows(workflows):\n",
    "    combined_workflow = {\"tasks\": []}\n",
    "    max_task_id = 0\n",
    "    new_max_task_id = 0\n",
    "    for workflow in workflows:\n",
    "        for task in workflow['tasks']:\n",
    "            task['id'] += max_task_id\n",
    "            task['dependencies'] = [dep + max_task_id for dep in task['dependencies']]\n",
    "            new_max_task_id = max(new_max_task_id, task['id'])\n",
    "            combined_workflow['tasks'].append(task)\n",
    "        max_task_id = new_max_task_id\n",
    "\n",
    "    return combined_workflow\n",
    "\n",
    "\n",
    "def benchmark_request(graph):\n",
    "    form_data = {\n",
    "        'hardware': json.dumps(hardware),\n",
    "        'bgo_dag': json.dumps(input_dag),\n",
    "        'graph_benchmarks': graph,\n",
    "        'greenify': 'true'\n",
    "    }\n",
    "    response = requests.post(url + '/benchmark', data=form_data)\n",
    "    return dict(json.loads(response.text))\n",
    "\n",
    "graphs = ['actor-collaboration', 'amazon0601', 'flixster', 'higgs_social_network', 'patentcite', 'sx_stackoverflow_c2q']\n",
    "responses = [benchmark_request(graph) for graph in graphs]\n",
    "print(responses)\n",
    "# combine_workflows(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the server\n",
    "server.terminate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
