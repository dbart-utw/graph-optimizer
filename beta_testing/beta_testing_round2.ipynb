{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph-Optimizer beta testing round 2\n",
    "\n",
    "## Tool description\n",
    "In short, the Graph-Optimizer tool performs the following functions:\n",
    "- Predicts the execution time (in milliseconds) and energy consumption (in Joules) for a given BGO or DAG of BGOs on a specific hardware configuration.\n",
    "- Returns the model in symbolical form with graph properties as symbols or predicts execution times if the graph properties are specified.\n",
    "- This is done via an API where issuing a POST request to `<api_url>/models` with the BGO DAG and hardware configuration returns an annotated DAG with calibrated symbolical models. Calling `<api_url>/evaluate` with the BGO DAG, hardware configuration, and graph properties returns an annotated DAG with predicted execution times.d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What is a BGO?\n",
    "A BGO, or Basic Graph Operation, is an atomic graph operation, that can serve as a building block for constructing larger graph _workloads_.\n",
    "A single BGO can have multiple implementations, possibly targeting different hardware platforms (e.g., CPU or GPU).\n",
    "A workload is a Directed Acyclic Graph (DAG) of BGOs, where the nodes are BGOs and the edges represent data dependencies between them.\n",
    "An example of such a DAG is shown below:\n",
    "\n",
    "<img style=\"margin-bottom: -245px\" src=\"dag.svg\">\n",
    "\n",
    "In this DAG, we start with the Betweenness Centrality (BC) BGO, followed by the Breadth First Search (BFS) and Find Max BGO's. Finally, we have the Find Path BGO to conclude. This example is a workload that outputs the path from the root node to the node with the highest betweenness centrality, which is the most popular node in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance modeling\n",
    "\n",
    "#### Analytical modeling\n",
    "An implementation of a BGO can have a symbolic model that describes its execution time and energy consumption as functions of graph properties and hardware characteristics. Specifically, these hardware characteristics refer to the execution times of atomic operations in hardware, or operations considered to be atomic, such as reading a value from memory, writing a value to memory, performing an integer addition, and so on. These characteristics are obtained through _microbenchmarks_ run on the hardware where the BGO will be executed. The values obtained from the microbenchmarks are used to calibrate the symbolic models, which then provide the execution time and energy consumption of the BGO based solely on the graph properties. This approach allows for predicting the execution time and energy consumption of a BGO on a specific hardware configuration without needing to run the BGO on the hardware for any particular input graph.\n",
    "\n",
    "Such a calibrated model might look like this:\n",
    "\n",
    "$T_{BGO} = 561n \\times 924m + 91n^2$,\n",
    "\n",
    "where $n$ is the number of nodes in a graph, $m$ is the number of edges in the graph, and $T_{BGO}$ is the execution time of the BGO in milliseconds.\n",
    "\n",
    "#### Sampling based prediction\n",
    "\n",
    "#### Benchmarks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to use the Graph-Optimizer tool\n",
    "### Step 1: Specify input DAG of BGO's\n",
    "\n",
    "The first step in using Graph Optimizer is to specify the input DAG of BGO's. From the above list, select the BGO's you want to use and specify the input DAG.\n",
    "This DAG should include one or multiple BGOs and their dependencies. The BGO name should match the name of the BGO folder in the `models` directory.\n",
    "There are currently <n> BGO's available. These are:\n",
    "- bc (Betweenness Centrality)\n",
    "- pr (PageRank)\n",
    "- bfs (Breadth First Search)\n",
    "- find_max (Find Max)\n",
    "- find_path (Find Path)\n",
    "- \n",
    "\n",
    "The dependencies should be specified as a list of BGO id's that the current BGO depends on. For instance, consider the following example with multiple BGOs and dependencies, representing the DAG shown in the introduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_dag = [\n",
    "    {\n",
    "        \"id\": 0,\n",
    "        \"name\": \"pr\",\n",
    "        \"dependencies\": []\n",
    "    },\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"name\": \"find_max\",\n",
    "        \"dependencies\": [0]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"name\": \"bfs\",\n",
    "        \"dependencies\": [0]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"name\": \"find_path\",\n",
    "        \"dependencies\": [1,2]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Step 2: Specify hardware configuration\n",
    "The hardware configuration aims to describe all available hardware components in a system or data center. The hardware information is used for the calibration of the performance models.\n",
    "\n",
    "To specify a custom hardware configuration, you need to provide the configuration in JSON format. This configuration should list all unique available hosts in the data center, including details about CPUs and, if applicable, GPUs. Running microbenchmarks is part of this process and is done automatically with a script. An example configuration is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hardware = {\n",
    "    \"hosts\": [\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"name\": \"H01\",\n",
    "            \"cpus\": {\n",
    "                \"id\": 1,\n",
    "                \"name\": \"intel xeon\",\n",
    "                \"clock_speed\": 2.10,\n",
    "                \"cores\": 16,\n",
    "                \"threads\": 32,\n",
    "                \"wattage\": 35,\n",
    "                \"amount\": 2,\n",
    "                \"benchmarks\": {\n",
    "                    \"T_int_add\": 1.739769,\n",
    "                    \"T_int_mult\": 0.2799874,\n",
    "                    \"T_int_gt\": 0.1126517,\n",
    "                    \"T_int_neq\": 0.2243598,\n",
    "                    \"T_float_add\": 0.4340147,\n",
    "                    \"T_float_sub\": 0.4194657,\n",
    "                    \"T_float_mult\": 0.421529,\n",
    "                    \"T_float_div\": 0.9876428,\n",
    "                    \"T_float_gt\": 0.1107689,\n",
    "                    \"T_q_push\": 7.381785,\n",
    "                    \"T_q_front\": 14.52919,\n",
    "                    \"T_q_pop\": 12.06002,\n",
    "                    \"T_heap_insert_max\": 37.04162,\n",
    "                    \"T_heap_extract_min\": 73.36278,\n",
    "                    \"T_heap_decrease_key\": 11.02303,\n",
    "                    \"T_push_back\": 5.958142,\n",
    "                    \"L1_linesize\": 64,\n",
    "                    \"L2_linesize\": 64,\n",
    "                    \"L3_linesize\": 64,\n",
    "                    \"T_L1_read\": 2.513609681995777,\n",
    "                    \"T_L2_read\": 5.099514592155088,\n",
    "                    \"T_L3_read\": 27.465506112424432,\n",
    "                    \"T_DRAM_read\": 66.34084759860137\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining hardware information\n",
    "\n",
    "The following code cell will fetch the hardware information for the system that this notebook is running on, and fill it in in the hardware configuration JSON object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "lscpu = {k.strip(): v.strip() for item in subprocess.check_output(['lscpu']).decode('utf-8').split('\\n') for k, _, v in [item.partition(':')]}\n",
    "hardware['hosts'][0]['cpus']['name'] = lscpu['Model name']\n",
    "hardware['hosts'][0]['cpus']['clock_speed'] = float(lscpu['CPU max MHz']) / 1000\n",
    "hardware['hosts'][0]['cpus']['cores'] = int(lscpu['Core(s) per socket'])\n",
    "hardware['hosts'][0]['cpus']['threads'] = int(lscpu['Thread(s) per core']) * hardware['hosts'][0]['cpus']['cores']\n",
    "hardware['hosts'][0]['cpus']['amount'] = int(lscpu['Socket(s)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### Automated microbenchmarks\n",
    "\n",
    "Running the following python cell will run the microbenchmarks on your machine (this should take a couple of seconds, probably no longer than a minute), and insert them into the hardware configuration.\n",
    "\n",
    "The resulting values are the measured values for each operation in nanoseconds.\n",
    "\n",
    "**Note**: for running the microbenchmarks, g++ is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "from benchmarks.microbenchmarks import all_benchmarks\n",
    "import json\n",
    "\n",
    "# Run microbenchmarks on this machine\n",
    "local_benchmarks = all_benchmarks()\n",
    "\n",
    "# Pretty print the benchmarks\n",
    "print(json.dumps(local_benchmarks, indent=4))\n",
    "\n",
    "# Assign obtained values to the hardware description\n",
    "hardware[\"hosts\"][0][\"cpus\"][\"benchmarks\"] = local_benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run the prediction server\n",
    "\n",
    "The next step is running the prediction server, and getting the performance models for the input DAG for your specific hardware configurations.\n",
    "This step can be divided into two subtasks. First we start the prediction server, and then we submit a POST request to the server to get the performance models.\n",
    "\n",
    "1. Start the server using flask, by running the following command from the root directory:\n",
    "    ```bash\n",
    "    flask --app api/api.py run\n",
    "    ```\n",
    "    _(If you are using windows, use `python -m flask --app api/api.py run`)_\n",
    "\n",
    "    This will start the server on `localhost:5000`. Use the following command to start the server on a different port:\n",
    "    ```bash\n",
    "    flask --app api/api.py run --port <port_number>\n",
    "    ```\n",
    "    You can either do this step, or run the python cell below, which will start the server for you.\n",
    "    - **Note**: If you run the server via the cell below, make sure to wait for the server to start before proceeding to the next cells.\n",
    "    - If you are finished with the experiments, you can stop the server by running the last cell in this document, which will terminate the server.\n",
    "2. Run the prediction by submitting a POST request to the api\n",
    "    - For obtaining the calibrated symbolical models, issue a post request to `localhost:<port_number>/models`, with the following post data:\n",
    "        - \"input_dag\": The input BGO DAG in JSON format, as a string.\n",
    "        - \"hardware\": The hardware configuration in JSON format, as a string.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start the server\n",
    "import multiprocessing\n",
    "\n",
    "port = 5000\n",
    "url = f'http://localhost:{port}/'\n",
    "\n",
    "def run_server():\n",
    "    !flask --app api/api.py run --port {port}\n",
    "\n",
    "server = multiprocessing.Process(target=run_server)\n",
    "server.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### Interpreting the results\n",
    "\n",
    "When submitting the post request to the API, the response will be the input DAG, but annotated with the calibrated symbolical models. The models will be in the form of a string representing a mathematical formula, with the graph properties as parameters.\n",
    "\n",
    "For demonstration purposes, a dropdown and slider are provided below, which allow you to change the microbenchmarking parameters and see the impact they have on the performance models.\n",
    "\n",
    "#### Submit a request to the prediction server by executing the cells below, and observe how the models change when altering the microbenchmarking parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def models_request():\n",
    "    form_data = {\n",
    "        'hardware': json.dumps(hardware),\n",
    "        'bgo_dag': json.dumps(input_dag)\n",
    "    }\n",
    "    models_response = requests.post(url + '/models', data=form_data)\n",
    "    print(models_response.text)\n",
    "    return models_response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dropdown and slider functionality\n",
    "from ipywidgets import interact, dlink, Dropdown, FloatSlider, Button\n",
    "from IPython.display import Markdown, display, clear_output\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "clear_output()\n",
    "\n",
    "microbenchmarks = hardware['hosts'][0]['cpus']['benchmarks']\n",
    "microbenchmark_name = list(microbenchmarks.keys())[0]\n",
    "dropdown = Dropdown(options=microbenchmarks.keys(), description='Variable')\n",
    "slider = FloatSlider(min=0, max=200, step=0.01, description='Value', value=microbenchmarks[microbenchmark_name])\n",
    "response = None\n",
    "\n",
    "def save_config(arg):\n",
    "    now = datetime.now().strftime('%Y-%m-%d.%H.%M.%S')\n",
    "    dir_name = f'./saved_configs/{now}/'\n",
    "    os.makedirs(dir_name, exist_ok=False)\n",
    "    with open(f'{dir_name}/hardware.json', 'w') as hw_file:\n",
    "        hw_file.write(json.dumps(hardware))\n",
    "    with open(f'{dir_name}/models.json', 'w') as model_file:\n",
    "        model_file.write(json.dumps(response))\n",
    "    print(f'Configuration saved to {dir_name} directory')\n",
    "\n",
    "\n",
    "def set_microbenchmark_name(variable):\n",
    "    global microbenchmark_name\n",
    "    microbenchmark_name = variable\n",
    "\n",
    "\n",
    "def update_slider_value(x):\n",
    "    global microbenchmark_name\n",
    "    microbenchmark_name = x\n",
    "    slider.value = microbenchmarks[x]\n",
    "    return slider.value\n",
    "\n",
    "\n",
    "def update_microbenchmark_value(value):\n",
    "    global response\n",
    "    hardware['hosts'][0]['cpus']['benchmarks'][microbenchmark_name] = value\n",
    "    response = json.loads(models_request())\n",
    "    return response\n",
    "\n",
    "\n",
    "dlink((dropdown, 'value'), (slider, 'value'), update_slider_value)\n",
    "display(Markdown('### Change the value of certain microbenchmarks, and see how they impact the performance models'))\n",
    "save_button = Button(description='Save configuration')\n",
    "save_button.on_click(save_config)\n",
    "display(save_button)\n",
    "interact(set_microbenchmark_name, variable=dropdown)\n",
    "interact(update_microbenchmark_value, value=slider);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Specify graph characteristics _(optional)_\n",
    "\n",
    "The final step is to specify the graph characteristics of a specific graph for which you want to predict the execution time. This can be done by submitting a POST request to the API with the input DAG, hardware configuration, and graph properties. The API will return the input DAG annotated with the predicted execution times.\n",
    "\n",
    "The graph properties are expressed in a simple JSON format of which an example is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_props = {\n",
    "    \"n\": 15763,\n",
    "    \"m\": 171206,\n",
    "    \"average_degree\": 21,\n",
    "    \"directed\": False,\n",
    "    \"weighted\": False,\n",
    "    \"clustering_coefficient\": 0.0132526,\n",
    "    \"triangle_count\": 591156,\n",
    "    \"s\": 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be useful when analyzing theoretical performance of an algorithm on non-existing graphs. However, when predicting performance for a specific graph, a graph file can be given, of which the properties are automatically extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyfilechooser import FileChooser\n",
    "\n",
    "fc = FileChooser('data/beta_testing')\n",
    "fc.filter_pattern = '*.mtx'\n",
    "display(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import scipy as sp\n",
    "import time\n",
    "\n",
    "graph_name = fc.selected\n",
    "g = nx.from_scipy_sparse_array(sp.io.mmread(graph_name))\n",
    "\n",
    "# Extract graph properties from file\n",
    "n = len(g.nodes())\n",
    "m = len(g.edges())\n",
    "extracted_properties = {\n",
    "    \"n\": n,\n",
    "    \"m\": m,\n",
    "    \"average_degree\": n / m,\n",
    "    \"directed\": g.is_directed(),\n",
    "    \"weighted\": nx.is_weighted(g),\n",
    "    \"diameter\": nx.diameter(g),\n",
    "    \"clustering_coefficient\": nx.average_clustering(g),\n",
    "    \"triangle_count\": sum(nx.triangles(g).values()) // 3\n",
    "}\n",
    "\n",
    "print(extracted_properties)\n",
    "graph_props = extracted_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_request():\n",
    "    form_data = {\n",
    "        'hardware': json.dumps(hardware),\n",
    "        'bgo_dag': json.dumps(input_dag),\n",
    "        'graph_props': json.dumps(graph_props)\n",
    "    }\n",
    "    evaluate_response = requests.post(url + '/evaluate', data=form_data)\n",
    "    print(evaluate_response.text)\n",
    "\n",
    "def set_num_nodes(value):\n",
    "    graph_props[\"n\"] = value\n",
    "    evaluate_request()\n",
    "\n",
    "display(Markdown('### Change the number of nodes in the graph, and see how it impacts the performance and energy predictions'))\n",
    "interact(set_num_nodes, value=FloatSlider(min=100, max=100000, step=1, description='#nodes', value=graph_props[\"n\"]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling based performance prediction\n",
    "Explanation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 0.1\n",
    "sampled_graph_name = 'test.mtx'\n",
    "!./sampling/main {graph_name} {sampling_rate} {sampled_graph_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(bgo, graph_file):\n",
    "    result = subprocess.check_output(['python3', './autobench/run_bench.py', bgo, '--data', f'G={graph_file}']).decode('utf-8').rstrip().split('\\n')[-2:]\n",
    "    values = dict(zip(*map(lambda x: x.split(','), result)))\n",
    "\n",
    "    return values['runtime_ns'], values['energy_joules']\n",
    "\n",
    "def run_and_print(bgo, graph_file):\n",
    "    time, energy = run_benchmark(bgo, graph_file)\n",
    "    print(f\"Running {bgo} on {graph_file} took {time}ns and {energy}Joules\")\n",
    "    \n",
    "run_and_print('./bgo/pr/CPU/pr_sequential', sampled_graph_name)\n",
    "run_and_print('./bgo/pr/CPU/pr_sequential', graph_name)\n",
    "\n",
    "run_and_print('./bgo/pr/GPU/edgelist', sampled_graph_name)\n",
    "run_and_print('./bgo/pr/GPU/edgelist', graph_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the server\n",
    "server.terminate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
